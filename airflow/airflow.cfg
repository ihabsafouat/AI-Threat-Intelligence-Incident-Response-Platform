[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
# Set this to True if you want to enable remote logging
remote_logging = False

# Users must supply an owner at the DAG level, otherwise the DAG won't be
# discovered by the airflow scheduler
dag_file_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = LocalExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow

# The encoding for the databases
sql_engine_encoding = utf-8

# If SqlAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
sql_alchemy_max_overflow = 10

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite.
sql_alchemy_pool_recycle = 3600

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# The maximum number of active DAG runs
max_active_runs = 16

# Whether to load the DAG examples that ship with Airflow. It's good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = False

# Where your Airflow plugins are stored
plugins_folder = /opt/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = 

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import while filling the DagBag
dagbag_import_timeout = 30.0

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a `run_as_user` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
default_impersonation = 

# The default owner assigned to each new operator, unless
# provided explicitly or passed via `default_args`
default_owner = airflow

# The default connections assigned to each new operator, unless
# provided explicitly or passed via `default_args`
default_connections = 

# The default queue assigned to each new operator, unless
# provided explicitly or passed via `default_args`
default_queue = default

# Default timezone in case supplied date times are naive
# can be utc (default), system, or any IANA timezone string (e.g. Europe/Amsterdam)
default_timezone = UTC

# The custom xcom backend class
xcom_backend = airflow.models.xcom.BaseXCom

# If set to True, Airflow will include the full path of the task file in the
# log filename. This is useful for debugging, but can result in very long log
# names.
include_mpns = False

# If set to True, Airflow will skip tasks that are not at the head of their
# dependency chain. This can be useful to ensure data consistency and avoid
# wasting resources on tasks that will fail anyway.
catchup_by_default = False

# If set to True, Airflow will run scheduler heartbeat in a background thread.
# If you run airflow in a distributed manner and leave this on, you may
# experience resource deadlock.
enable_xcom_pickling = True

# When not using pools, tasks are run in the "default pool",
# whose size is guided by this config element
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_tasks_per_dag = 16

# The maximum number of active DAG runs
max_active_tasks = 16

# Known executor types
known_executors = airflow.executors.sequential_executor.SequentialExecutor,airflow.executors.local_executor.LocalExecutor,airflow.executors.celery_executor.CeleryExecutor,airflow.executors.celery_kubernetes_executor.CeleryKubernetesExecutor,airflow.executors.kubernetes_executor.KubernetesExecutor,airflow.executors.local_kubernetes_executor.LocalKubernetesExecutor

# Task runner
task_runner = StandardTaskRunner

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run.
max_threads = 2

# The scheduler will try to trigger no more than `scheduler_heartbeat_sec` 
# task instances per heartbeat
scheduler_heartbeat_sec = 5

# Number of seconds after which a DAG file is parsed. The DAG file is parsed
# every `dag_dir_list_interval` seconds. But to avoid getting stuck in a
# parsing loop, if more than `dag_file_processor_timeout` seconds have
# passed since the last time the DAG file was parsed, the file will be
# parsed again.
dag_file_processor_timeout = 50

# If set to True, Airflow will include the full path of the task file in the
# log filename. This is useful for debugging, but can result in very long log
# names.
dag_are_paused_at_creation = True 